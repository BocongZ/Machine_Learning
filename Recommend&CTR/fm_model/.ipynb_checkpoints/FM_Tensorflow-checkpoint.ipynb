{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.训练过程\n",
    "# \"放大\"错误样本,基学习器的个数==训练的迭代次数\n",
    "\n",
    "# 1.输出形式\n",
    "# 二分类:\n",
    "# 每个基学习器输出one-hot向量(长度==num_leaves)\n",
    "# one-hot to int\n",
    "# prediction = num_boost_iteration个整数(每个整数是来自一个基学习器的输出)\n",
    "\n",
    "# 思考: \n",
    "# 1.多分类 || 回归\n",
    "# GBDT的树的输出还是one-hot吗？？  \n",
    "# 2.第一个基学习器的特征的最明显,最后一个基学习器的是分类结果最准确的\n",
    "\n",
    "\n",
    "# 2.FM\n",
    "# 输入:libsvm格式,index:value\n",
    "# 2.1 方法1\n",
    "# GBDT的树的序号作为key,叶子节点的序号作为val。 e.g. 0:31,1:31,2:29,3:31,......\n",
    "# 2.2 方法2\n",
    "# 全局的叶子节点序号作为key,数字1作为val。 e.g. 31:1,63:1,95:1....,1023:1.....\n",
    "# 2.3 https://zhuanlan.zhihu.com/p/35046241\n",
    "# 原来我仿写的代码,树的序号的作为key,叶子节点做偏移的值作为val\n",
    "\n",
    "# 3.其他问题:\n",
    "# Xgboost: 输入的类别特征如何指定？（lightgbm指定类别特征十分方便）\n",
    "\n",
    "\n",
    "# 1.GBDT\n",
    "# 通过GBDT的输出 构造成FM的输入\n",
    "# 根节点到叶子节点的路径可以想象成特征组合\n",
    "# 2.FM\n",
    "# LR+二阶特征组合  https://blog.csdn.net/u014297722/article/details/89762616\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'xgboost.core.Booster'>\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"/media/data/cuixuange/Criteo_dataset/model/xgboost_dump.pickle\",\"rb\") as pickle_file: #read binary file mode=rb\n",
    "    xgbModel= pickle.load(pickle_file)\n",
    "print(type(xgbModel))\n",
    "\n",
    "# paramter参数信息转为dict\n",
    "import json\n",
    "xgboost_dump_parameter = json.load(open(\"/media/data/cuixuange/Criteo_dataset/model/xgboost_dump.parameter\", \"r\"))\n",
    "# print(xgboost_dump_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100009, 31)\n",
      "62\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "# #############一些关于GBDT输出的测试\n",
    "\n",
    "def test_predict():\n",
    "    preprocess_data_dir = \"/media/data/cuixuange/Criteo_dataset/xgboost_data/\"\n",
    "    df_train_ = pd.read_csv(preprocess_data_dir+\"valid_lgb.txt\", header=None, sep=\"\\t\")\n",
    "    # FIXME: 构造Dmatrix格式的样本集合时 feature_names不传递后面predict报错？ - 缺失值如何处理的呢？\n",
    "    X_train_ = xgb.DMatrix(df_train_.drop(0, axis=1).values,feature_names=[\"I1\",\"I2\",\"I3\",\"I4\",\"I5\",\"I6\",\"I7\",\"I8\",\"I9\",\"I10\",\"I11\",\"I12\",\"I13\",\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"C6\",\"C7\",\"C8\",\"C9\",\"C10\",\"C11\",\"C12\",\"C13\",\"C14\",\"C15\",\"C16\",\"C17\",\"C18\",\"C19\",\"C20\",\"C21\",\"C22\",\"C23\",\"C24\",\"C25\",\"C26\"])\n",
    "    ## FIXME:  pred_leaf参数含义？？  会输出每条数据落在哪些叶子上？？\n",
    "    valid_leaves= xgbModel.predict(X_train_, ntree_limit=xgbModel.best_iteration, pred_leaf=True)\n",
    "#     print(xgbModel.best_ntree_limit)\n",
    "#     print(xgbModel.best_iteration)\n",
    "\n",
    "    return valid_leaves\n",
    "\n",
    "# 二叉树节点的index值 从31-62是叶子节点（32个）\n",
    "valid_leaves = test_predict()\n",
    "print(valid_leaves.shape)   #row*num_trees\n",
    "print(valid_leaves.max())  #62  \n",
    "print(valid_leaves.min())  #31\n",
    "\n",
    "\n",
    "# def tree_info():\n",
    "#     print(len(xgboost_dump_parameter))# 32次迭代轮数 = 32颗树\n",
    "#     print(xgboost_dump_parameter[0])\n",
    "    \n",
    "# tree_info()\n",
    "\n",
    "############GBDT offset (32次迭代轮数 每次迭代的输出时30个叶子节点的one-hot形式)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_trees=32  # 和迭代次数是一样的\n",
    "num_leaves=32   # 所有的叶子节点个数\n",
    "\n",
    "train_path = \"/media/data/cuixuange/Criteo_dataset/xgboost_data/train_lgb.txt\"\n",
    "valid_path = \"/media/data/cuixuange/Criteo_dataset/xgboost_data/valid_lgb.txt\"\n",
    "test_path = \"/media/data/cuixuange/Criteo_dataset/xgboost_data/test_lgb.txt\"\n",
    "\n",
    "fm_output_tr = \"/media/data/cuixuange/Criteo_dataset/xgboost_data/xgboost2fm_data/xgb2fm_train.txt\"\n",
    "fm_output_va = \"/media/data/cuixuange/Criteo_dataset/xgboost_data/xgboost2fm_data/xgb2fm_valid.txt\"\n",
    "fm_output_te = \"/media/data/cuixuange/Criteo_dataset/xgboost_data/xgboost2fm_data/xgb2fm_test.txt\"\n",
    "\n",
    "def gbdt2fm():\n",
    "    df_train_ = pd.read_csv(train_path, header=None, sep=\"\\t\")\n",
    "    df_valid_ = pd.read_csv(valid_path, header=None, sep=\"\\t\")\n",
    "    df_test_= pd.read_csv(test_path, header=None, sep=\"\\t\")\n",
    "    \n",
    "    y_train_ = df_train_[0].values\n",
    "    y_valid_ = df_valid_[0].values    \n",
    "    # xgb.Dmatrix 的构造为什么需要名字呢？  \n",
    "    X_train_ = xgb.DMatrix(data=df_train_.drop(0, axis=1).values,feature_names=[\"I1\",\"I2\",\"I3\",\"I4\",\"I5\",\"I6\",\"I7\",\"I8\",\"I9\",\"I10\",\"I11\",\"I12\",\"I13\",\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"C6\",\"C7\",\"C8\",\"C9\",\"C10\",\"C11\",\"C12\",\"C13\",\"C14\",\"C15\",\"C16\",\"C17\",\"C18\",\"C19\",\"C20\",\"C21\",\"C22\",\"C23\",\"C24\",\"C25\",\"C26\"])\n",
    "    X_valid_ = xgb.DMatrix(data=df_valid_.drop(0, axis=1).values,feature_names=[\"I1\",\"I2\",\"I3\",\"I4\",\"I5\",\"I6\",\"I7\",\"I8\",\"I9\",\"I10\",\"I11\",\"I12\",\"I13\",\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"C6\",\"C7\",\"C8\",\"C9\",\"C10\",\"C11\",\"C12\",\"C13\",\"C14\",\"C15\",\"C16\",\"C17\",\"C18\",\"C19\",\"C20\",\"C21\",\"C22\",\"C23\",\"C24\",\"C25\",\"C26\"])\n",
    "    X_test_= xgb.DMatrix(df_test_.values,feature_names=[\"I1\",\"I2\",\"I3\",\"I4\",\"I5\",\"I6\",\"I7\",\"I8\",\"I9\",\"I10\",\"I11\",\"I12\",\"I13\",\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"C6\",\"C7\",\"C8\",\"C9\",\"C10\",\"C11\",\"C12\",\"C13\",\"C14\",\"C15\",\"C16\",\"C17\",\"C18\",\"C19\",\"C20\",\"C21\",\"C22\",\"C23\",\"C24\",\"C25\",\"C26\"])\n",
    "    \n",
    "    \n",
    "    ############# row=样本集数量  column=基学习器的迭代轮数\n",
    "    \"\"\"\n",
    "    设置参数pred_leaf=true的作用\n",
    "    the output will be a matrix of (nsample, ntrees) with each record indicating the predicted leaf index of each sample in each tree\n",
    "    \"\"\"\n",
    "    train_leaves= xgbModel.predict(X_train_, ntree_limit=xgbModel.best_ntree_limit, pred_leaf=True)\n",
    "    valid_leaves= xgbModel.predict(X_valid_, ntree_limit=xgbModel.best_ntree_limit, pred_leaf=True)\n",
    "    test_leaves= xgbModel.predict(X_test_, ntree_limit=xgbModel.best_ntree_limit, pred_leaf=True)\n",
    "    \n",
    "    ############offset concat_one_hot\n",
    "    for i in range(num_trees):\n",
    "        train_leaves[:,i] = (train_leaves[:,i]-31) + i*num_leaves\n",
    "        valid_leaves[:,i] = (valid_leaves[:,i]-31) + i*num_leaves\n",
    "        test_leaves[:,i] = (test_leaves[:,i]-31) + i*num_leaves\n",
    "    \n",
    "    with open(fm_output_tr,\"w\") as out_train:\n",
    "        for idx in range(len(df_train_)):            \n",
    "            out_train.write((str(y_train_[idx]) + '\\t'))  #label\n",
    "            out_train.write('\\t'.join(['{}:{}'.format(val, 1) for ii,val in enumerate(train_leaves[idx]) if float(val) != 0 ]) + '\\n')\n",
    "            \n",
    "    with open(fm_output_va,\"w\") as out_valid:\n",
    "        for idx in range(len(df_valid_)):            \n",
    "            out_valid.write((str(y_valid_[idx]) + '\\t'))  #label\n",
    "            out_valid.write('\\t'.join(['{}:{}'.format(val, 1) for ii,val in enumerate(valid_leaves[idx]) if float(val) != 0 ]) + '\\n')\n",
    "\n",
    "    with open(fm_output_te,\"w\") as out_test:\n",
    "        for idx in range(len(df_test_)):                   \n",
    "            out_test.write('\\t'.join(['{}:{}'.format(val, 1) for ii,val in enumerate(test_leaves[idx]) if float(val) != 0 ]) + '\\n')\n",
    "        \n",
    "    \n",
    "gbdt2fm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/data/cuixuange/Anaconda/home/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "#### tensorflow 实现 FM\n",
    "import tensorflow as tf\n",
    "num_trees = 32\n",
    "num_leaves = 32\n",
    "\n",
    "# 定义基类模型\n",
    "DTYPE = tf.float32\n",
    "dtype = DTYPE\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.sess = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.layer_keeps = None\n",
    "        self.vars = None\n",
    "        self.keep_prob_train = None\n",
    "        self.keep_prob_test = None\n",
    "\n",
    "    # run model\n",
    "    def run(self, fetches, X=None, y=None, mode='train'):\n",
    "            print(fetches,X.shape,y.shape)\n",
    "            # 通过feed_dict传入数据\n",
    "            feed_dict = {}\n",
    "            if type(self.X) is list:\n",
    "                for i in range(len(X)):\n",
    "                    feed_dict[self.X[i]] = X[i]\n",
    "            else:\n",
    "                feed_dict[self.X] = X\n",
    "            if y is not None:\n",
    "                feed_dict[self.y] = y\n",
    "            if self.layer_keeps is not None:\n",
    "                if mode == 'train':\n",
    "                    feed_dict[self.layer_keeps] = self.keep_prob_train\n",
    "                elif mode == 'test':\n",
    "                    feed_dict[self.layer_keeps] = self.keep_prob_test\n",
    "            #通过session.run去执行op\n",
    "            print(fetches)\n",
    "            print(feed_dict.shape,feed_dict)\n",
    "            print(1111111111111111111111111111111111111111)\n",
    "            return self.sess.run(fetches, feed_dict)\n",
    "\n",
    "    # 模型参数持久化\n",
    "    def dump(self, model_path):\n",
    "        var_map = {}\n",
    "        for name, var in self.vars.iteritems():\n",
    "            var_map[name] = self.run(var)\n",
    "        pkl.dump(var_map, open(model_path, 'wb'))\n",
    "        print('model dumped at', model_path)\n",
    "        \n",
    "        \n",
    "# 在tensorflow中初始化各种参数变量\n",
    "def init_var_map(init_vars, init_path=None):\n",
    "    if init_path is not None:\n",
    "        load_var_map = pkl.load(open(init_path, 'rb'))\n",
    "        print('load variable map from', init_path, load_var_map.keys())\n",
    "    var_map = {}\n",
    "    for var_name, var_shape, init_method, dtype in init_vars:\n",
    "        if init_method == 'zero':\n",
    "            var_map[var_name] = tf.Variable(tf.zeros(var_shape, dtype=dtype), name=var_name, dtype=dtype)\n",
    "        elif init_method == 'one':\n",
    "            var_map[var_name] = tf.Variable(tf.ones(var_shape, dtype=dtype), name=var_name, dtype=dtype)\n",
    "        elif init_method == 'normal':\n",
    "            var_map[var_name] = tf.Variable(tf.random_normal(var_shape, mean=0.0, stddev=STDDEV, dtype=dtype),\n",
    "                                            name=var_name, dtype=dtype)\n",
    "        elif init_method == 'tnormal':\n",
    "            var_map[var_name] = tf.Variable(tf.truncated_normal(var_shape, mean=0.0, stddev=STDDEV, dtype=dtype),\n",
    "                                            name=var_name, dtype=dtype)\n",
    "        elif init_method == 'uniform':\n",
    "            var_map[var_name] = tf.Variable(tf.random_uniform(var_shape, minval=MINVAL, maxval=MAXVAL, dtype=dtype),\n",
    "                                            name=var_name, dtype=dtype)\n",
    "        elif init_method == 'xavier':\n",
    "            maxval = np.sqrt(6. / np.sum(var_shape))\n",
    "            minval = -maxval\n",
    "            var_map[var_name] = tf.Variable(tf.random_uniform(var_shape, minval=minval, maxval=maxval, dtype=dtype),\n",
    "                                            name=var_name, dtype=dtype)\n",
    "        elif isinstance(init_method, int) or isinstance(init_method, float):\n",
    "            var_map[var_name] = tf.Variable(tf.ones(var_shape, dtype=dtype) * init_method, name=var_name, dtype=dtype)\n",
    "        elif init_method in load_var_map:\n",
    "            if load_var_map[init_method].shape == tuple(var_shape):\n",
    "                var_map[var_name] = tf.Variable(load_var_map[init_method], name=var_name, dtype=dtype)\n",
    "            else:\n",
    "                print('BadParam: init method', init_method, 'shape', var_shape, load_var_map[init_method].shape)\n",
    "        else:\n",
    "            print('BadParam: init method', init_method)\n",
    "    return var_map\n",
    "\n",
    "\n",
    "# 不同的优化器选择\n",
    "def get_optimizer(opt_algo, learning_rate, loss):\n",
    "    if opt_algo == 'adaldeta':\n",
    "        return tf.train.AdadeltaOptimizer(learning_rate).minimize(loss)\n",
    "    elif opt_algo == 'adagrad':\n",
    "        return tf.train.AdagradOptimizer(learning_rate).minimize(loss)\n",
    "    elif opt_algo == 'adam':\n",
    "        return tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    elif opt_algo == 'ftrl':\n",
    "        return tf.train.FtrlOptimizer(learning_rate).minimize(loss)\n",
    "    elif opt_algo == 'gd':\n",
    "        return tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    elif opt_algo == 'padagrad':\n",
    "        return tf.train.ProximalAdagradOptimizer(learning_rate).minimize(loss)\n",
    "    elif opt_algo == 'pgd':\n",
    "        return tf.train.ProximalGradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    elif opt_algo == 'rmsprop':\n",
    "        return tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
    "    else:\n",
    "        return tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "# csr转成输入格式\n",
    "def csr_2_input(csr_mat):\n",
    "    if not isinstance(csr_mat, list):\n",
    "        coo_mat = csr_mat.tocoo()\n",
    "        indices = np.vstack((coo_mat.row, coo_mat.col)).transpose()\n",
    "        values = csr_mat.data\n",
    "        shape = csr_mat.shape\n",
    "        return indices, values, shape\n",
    "    else:\n",
    "        inputs = []\n",
    "        for csr_i in csr_mat:\n",
    "            inputs.append(csr_2_input(csr_i))\n",
    "        return inputs\n",
    "    \n",
    "# 数据切片\n",
    "# csr_data is tuple, csr_data[0] is data, csr_data[1] is label\n",
    "def my_slice(csr_data, start=0, size=-1):\n",
    "    if not isinstance(csr_data[0], list):\n",
    "        if size == -1 or start + size >= csr_data[0].shape[0]:\n",
    "            slc_data = csr_data[0][start:]\n",
    "            slc_labels = csr_data[1][start:]\n",
    "        else:\n",
    "            slc_data = csr_data[0][start:start + size]\n",
    "            slc_labels = csr_data[1][start:start + size]\n",
    "    else:\n",
    "        if size == -1 or start + size >= csr_data[0][0].shape[0]:\n",
    "            slc_data = []\n",
    "            for d_i in csr_data[0]:\n",
    "                slc_data.append(d_i[start:])\n",
    "            slc_labels = csr_data[1][start:]\n",
    "        else:\n",
    "            slc_data = []\n",
    "            for d_i in csr_data[0]:\n",
    "                slc_data.append(d_i[start:start + size])\n",
    "            slc_labels = csr_data[1][start:start + size]\n",
    "    return csr_2_input(slc_data), slc_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 11272363548444637295\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 12042859316\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 10648784071341473952\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0, compute capability: 5.2\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "\n",
    "class FM(Model):\n",
    "    def __init__(self, input_dim=None, output_dim=1, factor_order=10, init_path=None, opt_algo='gd', learning_rate=1e-2,\n",
    "                 l2_w=0, l2_v=0, random_seed=None):\n",
    "        Model.__init__(self)\n",
    "        \n",
    "        with tf.device('/gpu:0'):\n",
    "            # 一次、二次交叉、偏置项\n",
    "            init_vars = [('w', [input_dim, output_dim], 'xavier', dtype),\n",
    "                         ('v', [input_dim, factor_order], 'xavier', dtype),\n",
    "                         ('b', [output_dim], 'zero', dtype)]\n",
    "            self.graph = tf.Graph()\n",
    "            with self.graph.as_default():\n",
    "                if random_seed is not None:\n",
    "                    tf.set_random_seed(random_seed)\n",
    "                self.X = tf.sparse_placeholder(dtype)\n",
    "                self.y = tf.placeholder(dtype)\n",
    "                self.vars = init_var_map(init_vars, init_path)\n",
    "\n",
    "                w = self.vars['w']\n",
    "                v = self.vars['v']\n",
    "                b = self.vars['b']\n",
    "\n",
    "                # [(x1+x2+x3)^2 - (x1^2+x2^2+x3^2)]/2\n",
    "                # 先计算所有的交叉项，再减去平方项(自己和自己相乘)\n",
    "                X_square = tf.SparseTensor(self.X.indices, tf.square(self.X.values), tf.to_int64(tf.shape(self.X)))\n",
    "                xv = tf.square(tf.sparse_tensor_dense_matmul(self.X, v))\n",
    "                p = 0.5 * tf.reshape(\n",
    "                    tf.reduce_sum(xv - tf.sparse_tensor_dense_matmul(X_square, tf.square(v)), 1),\n",
    "                    [-1, output_dim])\n",
    "                xw = tf.sparse_tensor_dense_matmul(self.X, w)\n",
    "                logits = tf.reshape(xw + b + p, [-1])\n",
    "                self.y_prob = tf.sigmoid(logits)\n",
    "\n",
    "                self.loss = tf.reduce_mean(\n",
    "                    tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=self.y)) + \\\n",
    "                            l2_w * tf.nn.l2_loss(xw) + \\\n",
    "                            l2_v * tf.nn.l2_loss(xv)\n",
    "                self.optimizer = get_optimizer(opt_algo, learning_rate, self.loss)\n",
    "\n",
    "                #GPU设定\n",
    "                config = tf.ConfigProto(device_count = {'GPU': 1},log_device_placement=True)\n",
    "                config.gpu_options.allow_growth = True\n",
    "                self.sess = tf.Session(config=config)\n",
    "                # 图中所有variable初始化\n",
    "                tf.global_variables_initializer().run(session=self.sess)\n",
    "            \n",
    "#     run model\n",
    "#     fetches = [model.optimizer, model.loss]\n",
    "    def run(self, fetches, X=None, y=None, mode='train'):\n",
    "            # 通过feed_dict传入数据\n",
    "            feed_dict = {}\n",
    "            if type(self.X) is list:\n",
    "                for i in range(len(X)):\n",
    "                    feed_dict[self.X[i]] = X[i]\n",
    "            else:\n",
    "                feed_dict[self.X] = X\n",
    "            if y is not None:\n",
    "                feed_dict[self.y] = y\n",
    "            if self.layer_keeps is not None:\n",
    "                if mode == 'train':\n",
    "                    feed_dict[self.layer_keeps] = self.keep_prob_train\n",
    "                elif mode == 'test':\n",
    "                    feed_dict[self.layer_keeps] = self.keep_prob_test\n",
    "            #通过session.run去执行op\n",
    "            return self.sess.run(fetches=fetches, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "\n",
    "INPUT_DIM=num_trees*num_leaves  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取libsvm格式数据成稀疏矩阵形式\n",
    "# 0 5:1 9:1 140858:1 445908:1 446177:1 446293:1 449140:1 490778:1 491626:1 491634:1 491641:1 491645:1 491648:1 491668:1 491700:1 491708:1\n",
    "def read_data(file_name):\n",
    "    X = []\n",
    "    D = []\n",
    "    y = []\n",
    "    with open(file_name) as fin:\n",
    "        for line in fin:\n",
    "            fields = line.strip().split(\"\\t\")\n",
    "            y_i = int(fields[0])\n",
    "            X_i = [int(x.split(':')[0]) for x in fields[1:]]\n",
    "            D_i = [int(x.split(':')[1]) for x in fields[1:]]\n",
    "            y.append(y_i)\n",
    "            X.append(X_i)\n",
    "            D.append(D_i)\n",
    "    y = np.reshape(np.array(y), [-1])\n",
    "    X = libsvm_2_coo(zip(X, D), (len(X), INPUT_DIM)).tocsr()\n",
    "    return X, y\n",
    "\n",
    "def shuffle(data):\n",
    "    X, y = data\n",
    "    ind = np.arange(X.shape[0])\n",
    "    for i in range(3):\n",
    "        np.random.shuffle(ind)\n",
    "    return X[ind], y[ind]\n",
    "\n",
    "# 工具函数，libsvm格式转成coo稀疏存储格式\n",
    "def libsvm_2_coo(libsvm_data, shape):\n",
    "    coo_rows = []\n",
    "    coo_cols = []\n",
    "    coo_data = []\n",
    "    n = 0\n",
    "    for x, d in libsvm_data:\n",
    "        coo_rows.extend([n] * len(x))\n",
    "        coo_cols.extend(x)\n",
    "        coo_data.extend(d)\n",
    "        n += 1\n",
    "    coo_rows = np.array(coo_rows)\n",
    "    coo_cols = np.array(coo_cols)\n",
    "    coo_data = np.array(coo_data)\n",
    "#     print(coo_data.shape)\n",
    "#     print(coo_rows.shape)\n",
    "#     print(coo_cols.shape)\n",
    "#     print(shape)\n",
    "    return coo_matrix((coo_data, (coo_rows, coo_cols)), shape=shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "train_file = '/media/data/cuixuange/Criteo_dataset/xgboost_data/xgboost2fm_data/xgb2fm_train.txt'\n",
    "valid_file = '/media/data/cuixuange/Criteo_dataset/xgboost_data/xgboost2fm_data/xgb2fm_valid.txt'\n",
    "\n",
    "input_dim = INPUT_DIM\n",
    "# train_data = read_data(train_file)   # tuple[0] is data,tuple[1] is label\n",
    "# train_data = shuffle(train_data)\n",
    "# valid_data = read_data(valid_file)\n",
    "# pkl.dump(train_data, open('/media/data/cuixuange/Criteo_dataset/xgboost_data/xgboost2fm_data/pkl/train.pkl', 'wb'))\n",
    "# pkl.dump(valid_data, open('/media/data/cuixuange/Criteo_dataset/xgboost_data/xgboost2fm_data/pkl/valid.pkl', 'wb'))\n",
    "train_data = pkl.load(open('/media/data/cuixuange/Criteo_dataset/xgboost_data/xgboost2fm_data/pkl/train.pkl', 'rb'))\n",
    "valid_data = pkl.load(open('/media/data/cuixuange/Criteo_dataset/xgboost_data/xgboost2fm_data/pkl/valid.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size: (899991, 1024)\n",
      "valid data size: (100009, 1024)\n",
      "train_data type: <class 'scipy.sparse.csr.csr_matrix'>\n",
      "one-sample data: [[0 0 0 ... 0 0 0]] 1024\n",
      "one-sample label: 1\n",
      "{'input_dim': 1024, 'factor_order': 10, 'opt_algo': 'gd', 'learning_rate': 0.1, 'l2_w': 0.001, 'l2_v': 0.001}\n",
      "training FM...\n",
      "[0]\ttraining...\n",
      "[0]\tevaluating...\n",
      "(899991,) 899991\n",
      "[0]\tloss (with l2 norm):0.555440\ttrain-auc: 0.722465\teval-auc: 0.716794\n",
      "[1]\ttraining...\n",
      "[1]\tevaluating...\n",
      "(899991,) 899991\n",
      "[1]\tloss (with l2 norm):0.530968\ttrain-auc: 0.722695\teval-auc: 0.717237\n",
      "[2]\ttraining...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print('train data size:', train_data[0].shape)\n",
    "print('valid data size:', valid_data[0].shape)\n",
    "print(\"train_data type:\", type(train_data[0]))\n",
    "print(\"one-sample data:\", train_data[0][0].toarray(),len(train_data[0][0].toarray()[0]))\n",
    "print(\"one-sample label:\", train_data[1][0])\n",
    "\n",
    "# # 训练集与测试集\n",
    "train_size = train_data[0].shape[0]\n",
    "valid_size = valid_data[0].shape[0]\n",
    "# num_feas = len(FIELD_SIZES)\n",
    "\n",
    "# 超参数设定\n",
    "min_round = 1\n",
    "num_round = 32\n",
    "early_stop_round = 5\n",
    "batch_size = 1024\n",
    "\n",
    "# field_sizes = FIELD_SIZES\n",
    "# field_offsets = FIELD_OFFSETS\n",
    "\n",
    "# FM参数设定\n",
    "fm_params = {\n",
    "    'input_dim': input_dim,\n",
    "    'factor_order': 10,   # 隐向量的长度\n",
    "    'opt_algo': 'gd',\n",
    "    'learning_rate': 0.1,\n",
    "    'l2_w': 0.001,\n",
    "    'l2_v': 0.001,\n",
    "}\n",
    "print(fm_params)\n",
    "model = FM(**fm_params)\n",
    "print(\"training FM...\")\n",
    "\n",
    "def train(model):\n",
    "    history_score = []\n",
    "    for i in range(num_round):\n",
    "        # 同样是优化器和损失两个op\n",
    "        fetches = [model.optimizer, model.loss]\n",
    "        if batch_size > 0:\n",
    "            ls = []\n",
    "            print('[%d]\\ttraining...' % i)\n",
    "            for j in range(int(train_size / batch_size + 1)):\n",
    "                X_i, y_i = my_slice(train_data, j * batch_size, batch_size)\n",
    "#                 print(\" X_i, y_i:\",type( X_i),type( y_i))\n",
    "#                 print(X_i)\n",
    "#                 print(y_i)\n",
    "                # 训练\n",
    "                _, l = model.run(fetches, X_i, y_i)\n",
    "                ls.append(l)\n",
    "        elif batch_size == -1:\n",
    "            X_i, y_i = my_slice(train_data)\n",
    "            _, l = model.run(fetches, X_i, y_i)\n",
    "            ls = [l]\n",
    " \n",
    "        train_preds = []\n",
    "        print('[%d]\\tevaluating...' % i)\n",
    "        for j in range(int(train_size / 10000 + 1)):\n",
    "            X_i, _ = my_slice(train_data, j * 10000, 10000)\n",
    "#             print(\"X_i.shape:\",X_i[0].shape)\n",
    "            preds = model.run(model.y_prob, X_i, mode='test')\n",
    "#             print(\"preds length:\",len(preds))\n",
    "            train_preds.extend(preds)\n",
    "            \n",
    "        test_preds = []\n",
    "        for j in range(int(valid_size / 10000 + 1)):\n",
    "            X_i, _ = my_slice(valid_data, j * 10000, 10000)\n",
    "            preds = model.run(model.y_prob, X_i, mode='test')\n",
    "            test_preds.extend(preds)\n",
    "                    \n",
    "            \n",
    "        print(train_data[1].shape,len(train_preds))\n",
    "        train_score = roc_auc_score(train_data[1], train_preds)\n",
    "        test_score = roc_auc_score(valid_data[1], test_preds)\n",
    "        print('[%d]\\tloss (with l2 norm):%f\\ttrain-auc: %f\\teval-auc: %f' % (i, np.mean(ls), train_score, test_score))\n",
    "        history_score.append(test_score)\n",
    "        if i > min_round and i > early_stop_round:\n",
    "            if np.argmax(history_score) == i - early_stop_round and history_score[-1] - history_score[\n",
    "                        -1 * early_stop_round] < 1e-5:\n",
    "                print('early stop\\nbest iteration:\\n[%d]\\teval-auc: %f' % (\n",
    "                    np.argmax(history_score), np.max(history_score)))\n",
    "                break\n",
    "                \n",
    "                \n",
    "\n",
    "train(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tendorflow",
   "language": "python",
   "name": "tendorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
