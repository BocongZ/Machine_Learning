{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集的预处理\n",
    "# 1.GBDT形式\n",
    "# label,13个整数特征dense,26个类别特征sparse\n",
    "#e.g. 0 0.05 0.004983 0.05 0 0.021594 0.008 0.15 0.04 0.362 0.166667 0.2 0 0.04 2 3 0 0 1 1 0 3 1 0 0 0 0 3 0 0 1 4 1 3 0 0 2 0 1 0\n",
    "\n",
    "# 总结:\n",
    "# 1.对于数值特征，我们将I1-I13转成0-1之间的小数\n",
    "# 2.类别特征我们将某类别使用次数少于cutoff（超参）的忽略掉. \n",
    "# e.g. 对于类别C1,由特征a,特征c,特征f(出现次数大于cutoff)组成。C1:a,c,f。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error#均方误差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\"\"\"\n",
    "用来序列化 反序列化对象\n",
    "\"\"\"\n",
    "def save_params(params):\n",
    "    pickle.dump(params, open('params.p', 'wb'))\n",
    "\n",
    "def load_params():\n",
    "    return pickle.load(open('params.p', mode='rb'))\n",
    "\n",
    "def save_params_with_name(params, name):\n",
    "    pickle.dump(params, open('{}.p'.format(name), 'wb'))\n",
    "\n",
    "def load_params_with_name(name):\n",
    "    return pickle.load(open('{}.p'.format(name), mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对于连续int类型 和 categorial类型的预处理\n",
    "#轮子来源:https://zhuanlan.zhihu.com/p/32500652"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 13 integer features and 26 categorical features\n",
    "continous_features = range(1, 14)\n",
    "categorial_features = range(14, 40)\n",
    "\n",
    "# 整数的每一个特征的总数的95% clip_point\n",
    "# Clip integer features. The clip point for each integer feature\n",
    "# is derived from the 95% quantile of the total values in each feature\n",
    "continous_clip = [20, 600, 100, 50, 64000, 500, 100, 50, 500, 10, 10, 10, 50]\n",
    "\n",
    "class ContinuousFeatureGenerator:\n",
    "    \"\"\"\n",
    "    Normalize the integer features to [0, 1] by min-max normalization\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_feature):\n",
    "        self.num_feature = num_feature\n",
    "        self.min = [sys.maxsize] * num_feature\n",
    "        self.max = [-sys.maxsize] * num_feature\n",
    "\n",
    "    def build(self, datafile, continous_features):\n",
    "        with open(datafile, 'r') as f:\n",
    "            for line in f:\n",
    "                features = line.rstrip('\\n').split('\\t')\n",
    "                for i in range(0, self.num_feature):\n",
    "                    val = features[continous_features[i]]\n",
    "                    if val != '':\n",
    "                        val = int(val)\n",
    "                        if val > continous_clip[i]:\n",
    "                            val = continous_clip[i]\n",
    "                        self.min[i] = min(self.min[i], val)\n",
    "                        self.max[i] = max(self.max[i], val)\n",
    "\n",
    "    def gen(self, idx, val):\n",
    "        if val == '':\n",
    "            return 0.0\n",
    "        val = float(val)\n",
    "        return (val - self.min[idx]) / (self.max[idx] - self.min[idx])\n",
    "\n",
    "class CategoryDictGenerator:\n",
    "    \"\"\"\n",
    "    Generate dictionary for each of the categorical features\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_feature):\n",
    "        self.dicts = []\n",
    "        self.num_feature = num_feature\n",
    "        for i in range(0, num_feature):\n",
    "            self.dicts.append(collections.defaultdict(int))\n",
    "\n",
    "    def build(self, datafile, categorial_features, cutoff=0):\n",
    "        \"\"\"\n",
    "        1.统计每个类别下的所有特征出现次数\n",
    "        2.每个类别筛选出现次数大于cutoff的特征; 给这些特征编号\n",
    "        \"\"\"\n",
    "        with open(datafile, 'r') as f:\n",
    "            for line in f:\n",
    "                features = line.rstrip('\\n').split('\\t')\n",
    "                for i in range(0, self.num_feature):\n",
    "                    if features[categorial_features[i]] != '':\n",
    "                        self.dicts[i][features[categorial_features[i]]] += 1\n",
    "        for i in range(0, self.num_feature):\n",
    "            self.dicts[i] = filter(lambda x: x[1] >= cutoff,\n",
    "                                   self.dicts[i].items())\n",
    "\n",
    "            self.dicts[i] = sorted(self.dicts[i], key=lambda x: (-x[1], x[0]))\n",
    "            vocabs, _ = list(zip(*self.dicts[i]))\n",
    "            self.dicts[i] = dict(zip(vocabs, range(1, len(vocabs) + 1)))\n",
    "            self.dicts[i]['<unk>'] = 0\n",
    "\n",
    "    def gen(self, idx, key):\n",
    "        \"\"\"\n",
    "        出现次数低于cutoff的特征,return 0\n",
    "        否则return int(此整数意味着该特征key的类别index中的序号)\n",
    "        \"\"\"\n",
    "        if key not in self.dicts[idx]:     \n",
    "            res = self.dicts[idx]['<unk>']\n",
    "        else:\n",
    "            res = self.dicts[idx][key]\n",
    "        return res\n",
    "\n",
    "    #FIXME: map(func,iteratable) 对于迭代器调用func\n",
    "    #return (26个类别)每个类别下的有效特征数(sparse稀疏)\n",
    "    def dicts_sizes(self):\n",
    "        return list(map(len, self.dicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(datadir, outdir):\n",
    "    \"\"\"\n",
    "    All the 13 integer features are normalzied to continous values and these\n",
    "    continous features are combined into one vecotr with dimension 13.\n",
    "\n",
    "    Each of the 26 categorical features are one-hot encoded and all the one-hot\n",
    "    vectors are combined into one sparse binary vector.\n",
    "    \n",
    "    1.dists 连续整数特征\n",
    "    2.dicts 类别特征\n",
    "    \"\"\"\n",
    "    dists = ContinuousFeatureGenerator(len(continous_features))\n",
    "    dists.build(os.path.join(datadir, 'train.txt'), continous_features)\n",
    "\n",
    "    dicts = CategoryDictGenerator(len(categorial_features))\n",
    "    dicts.build(os.path.join(datadir, 'train.txt'), categorial_features, cutoff=200)#200 50\n",
    "\n",
    "    dict_sizes = dicts.dicts_sizes()\n",
    "    categorial_feature_offset = [0]\n",
    "    for i in range(1, len(categorial_features)):\n",
    "        offset = categorial_feature_offset[i - 1] + dict_sizes[i - 1]\n",
    "        categorial_feature_offset.append(offset)\n",
    "\n",
    "    random.seed(0)\n",
    "\n",
    "    # 90% of the data are used for training, and 10% of the data are used\n",
    "    # for validation.\n",
    "    train_ffm = open(os.path.join(outdir, 'train_ffm.txt'), 'w')\n",
    "    valid_ffm = open(os.path.join(outdir, 'valid_ffm.txt'), 'w')\n",
    "\n",
    "    train_lgb = open(os.path.join(outdir, 'train_lgb.txt'), 'w')\n",
    "    valid_lgb = open(os.path.join(outdir, 'valid_lgb.txt'), 'w')\n",
    "\n",
    "    with open(os.path.join(outdir, 'train.txt'), 'w') as out_train:\n",
    "        with open(os.path.join(outdir, 'valid.txt'), 'w') as out_valid:\n",
    "            with open(os.path.join(datadir, 'train.txt'), 'r') as f:\n",
    "                for line in f:\n",
    "                    features = line.rstrip('\\n').split('\\t')\n",
    "                    continous_feats = []\n",
    "                    continous_vals = []\n",
    "                    for i in range(0, len(continous_features)):\n",
    "\n",
    "                        val = dists.gen(i, features[continous_features[i]])\n",
    "                        continous_vals.append(\n",
    "                            \"{0:.6f}\".format(val).rstrip('0').rstrip('.'))\n",
    "                        continous_feats.append(\n",
    "                            \"{0:.6f}\".format(val).rstrip('0').rstrip('.'))#('{0}'.format(val))\n",
    "\n",
    "                    categorial_vals = []\n",
    "                    categorial_lgb_vals = []\n",
    "                    for i in range(0, len(categorial_features)):\n",
    "                        val = dicts.gen(i, features[categorial_features[i]]) + categorial_feature_offset[i]\n",
    "                        categorial_vals.append(str(val))\n",
    "                        val_lgb = dicts.gen(i, features[categorial_features[i]])\n",
    "                        categorial_lgb_vals.append(str(val_lgb))\n",
    "\n",
    "                    continous_vals = ','.join(continous_vals)\n",
    "                    categorial_vals = ','.join(categorial_vals)\n",
    "                    label = features[0]\n",
    "                    if random.randint(0, 9999) % 10 != 0:\n",
    "                        out_train.write(','.join(\n",
    "                            [continous_vals, categorial_vals, label]) + '\\n')\n",
    "                        train_ffm.write('\\t'.join(label) + '\\t')\n",
    "                        train_ffm.write('\\t'.join(\n",
    "                            ['{}:{}:{}'.format(ii, ii, val) for ii,val in enumerate(continous_vals.split(','))]) + '\\t')\n",
    "                        train_ffm.write('\\t'.join(\n",
    "                            ['{}:{}:1'.format(ii + 13, str(np.int32(val) + 13)) for ii, val in enumerate(categorial_vals.split(','))]) + '\\n')\n",
    "                        \n",
    "                        train_lgb.write('\\t'.join(label) + '\\t')\n",
    "                        train_lgb.write('\\t'.join(continous_feats) + '\\t')\n",
    "                        train_lgb.write('\\t'.join(categorial_lgb_vals) + '\\n')\n",
    "\n",
    "                    else:\n",
    "                        out_valid.write(','.join(\n",
    "                            [continous_vals, categorial_vals, label]) + '\\n')\n",
    "                        valid_ffm.write('\\t'.join(label) + '\\t')\n",
    "                        valid_ffm.write('\\t'.join(\n",
    "                            ['{}:{}:{}'.format(ii, ii, val) for ii,val in enumerate(continous_vals.split(','))]) + '\\t')\n",
    "                        valid_ffm.write('\\t'.join(\n",
    "                            ['{}:{}:1'.format(ii + 13, str(np.int32(val) + 13)) for ii, val in enumerate(categorial_vals.split(','))]) + '\\n')\n",
    "                                                \n",
    "                        valid_lgb.write('\\t'.join(label) + '\\t')\n",
    "                        valid_lgb.write('\\t'.join(continous_feats) + '\\t')\n",
    "                        valid_lgb.write('\\t'.join(categorial_lgb_vals) + '\\n')\n",
    "                        \n",
    "    train_ffm.close()\n",
    "    valid_ffm.close()\n",
    "\n",
    "    train_lgb.close()\n",
    "    valid_lgb.close()\n",
    "\n",
    "    test_ffm = open(os.path.join(outdir, 'test_ffm.txt'), 'w')\n",
    "    test_lgb = open(os.path.join(outdir, 'test_lgb.txt'), 'w')\n",
    "\n",
    "    with open(os.path.join(outdir, 'test.txt'), 'w') as out:\n",
    "        with open(os.path.join(datadir, 'test.txt'), 'r') as f:\n",
    "            for line in f:\n",
    "                features = line.rstrip('\\n').split('\\t')\n",
    "\n",
    "                continous_feats = []\n",
    "                continous_vals = []\n",
    "                for i in range(0, len(continous_features)):\n",
    "                    val = dists.gen(i, features[continous_features[i] - 1])\n",
    "                    continous_vals.append(\n",
    "                        \"{0:.6f}\".format(val).rstrip('0').rstrip('.'))\n",
    "                    continous_feats.append(\n",
    "                            \"{0:.6f}\".format(val).rstrip('0').rstrip('.'))#('{0}'.format(val))\n",
    "\n",
    "                categorial_vals = []\n",
    "                categorial_lgb_vals = []\n",
    "                for i in range(0, len(categorial_features)):\n",
    "                    val = dicts.gen(i,\n",
    "                                    features[categorial_features[i] -\n",
    "                                             1]) + categorial_feature_offset[i]\n",
    "                    categorial_vals.append(str(val))\n",
    "\n",
    "                    val_lgb = dicts.gen(i, features[categorial_features[i] - 1])\n",
    "                    categorial_lgb_vals.append(str(val_lgb))\n",
    "\n",
    "                continous_vals = ','.join(continous_vals)\n",
    "                categorial_vals = ','.join(categorial_vals)\n",
    "\n",
    "                out.write(','.join([continous_vals, categorial_vals]) + '\\n')\n",
    "                \n",
    "                test_ffm.write('\\t'.join(['{}:{}:{}'.format(ii, ii, val) for ii,val in enumerate(continous_vals.split(','))]) + '\\t')\n",
    "                test_ffm.write('\\t'.join(\n",
    "                    ['{}:{}:1'.format(ii + 13, str(np.int32(val) + 13)) for ii, val in enumerate(categorial_vals.split(','))]) + '\\n')\n",
    "                                                                \n",
    "                test_lgb.write('\\t'.join(continous_feats) + '\\t')\n",
    "                test_lgb.write('\\t'.join(categorial_lgb_vals) + '\\n')\n",
    "\n",
    "    test_ffm.close()\n",
    "    test_lgb.close()\n",
    "    return dict_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_sizes = preprocess('/media/data/cuixuange/Criteo_dataset/rawData','/media/data/cuixuange/Criteo_dataset/preprocess_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pickle.dump 序列化对象  encoder\n",
    "save_params_with_name((dict_sizes), 'dict_sizes')   #pickle.dump((dict_sizes), open('dict_sizes.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_sizes = load_params_with_name('dict_sizes') #pickle.load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(dict_sizes))\n",
    "print(type(dict_sizes))\n",
    "print(len(dict_sizes))  # 26个类别,每个类别下的有效特征数(sparse稀疏)\n",
    "print(dict_sizes[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs231n",
   "language": "python",
   "name": "cs231n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
