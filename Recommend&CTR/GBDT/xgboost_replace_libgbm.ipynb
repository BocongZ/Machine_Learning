{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error#均方误差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 13 integer features and 26 categorical features\n",
    "continous_features = range(1, 14)\n",
    "categorial_features = range(14, 40)\n",
    "\n",
    "# 整数的每一个特征的总数的95% clip_point\n",
    "# Clip integer features. The clip point for each integer feature\n",
    "# is derived from the 95% quantile of the total values in each feature\n",
    "continous_clip = [20, 600, 100, 50, 64000, 500, 100, 50, 500, 10, 10, 10, 50]\n",
    "\n",
    "class ContinuousFeatureGenerator:\n",
    "    \"\"\"\n",
    "    Normalize the integer features to [0, 1] by min-max normalization\n",
    "    \"\"\"\n",
    "    def __init__(self, num_feature):\n",
    "        self.num_feature = num_feature\n",
    "        self.min = [sys.maxsize] * num_feature\n",
    "        self.max = [-sys.maxsize] * num_feature\n",
    "\n",
    "    def build(self, datafile, continous_features):\n",
    "        with open(datafile, 'r') as f:\n",
    "            for line in f:\n",
    "                features = line.rstrip('\\n').split('\\t')\n",
    "                for i in range(0, self.num_feature):\n",
    "                    val = features[continous_features[i]]\n",
    "                    if val != '':\n",
    "                        val = int(val)\n",
    "                        if val > continous_clip[i]:\n",
    "                            val = continous_clip[i]\n",
    "                        self.min[i] = min(self.min[i], val)\n",
    "                        self.max[i] = max(self.max[i], val)\n",
    "\n",
    "    def gen(self, idx, val):\n",
    "        if val == '':\n",
    "            return 0.0\n",
    "        val = float(val)\n",
    "        return (val - self.min[idx]) / (self.max[idx] - self.min[idx])\n",
    "\n",
    "class CategoryDictGenerator:\n",
    "    \"\"\"\n",
    "    Generate dictionary for each of the categorical features\n",
    "    \"\"\"\n",
    "    def __init__(self, num_feature):\n",
    "        self.dicts = []\n",
    "        self.num_feature = num_feature\n",
    "        for i in range(0, num_feature):\n",
    "            self.dicts.append(collections.defaultdict(int))\n",
    "\n",
    "    def build(self, datafile, categorial_features, cutoff=0):\n",
    "        \"\"\"\n",
    "        1.统计每个类别下的所有特征出现次数\n",
    "        2.每个类别筛选出现次数大于cutoff的特征; 给这些特征编号\n",
    "        \"\"\"\n",
    "        with open(datafile, 'r') as f:\n",
    "            for line in f:\n",
    "                features = line.rstrip('\\n').split('\\t')\n",
    "                for i in range(0, self.num_feature):\n",
    "                    if features[categorial_features[i]] != '':\n",
    "                        self.dicts[i][features[categorial_features[i]]] += 1\n",
    "        for i in range(0, self.num_feature):\n",
    "            self.dicts[i] = filter(lambda x: x[1] >= cutoff,\n",
    "                                   self.dicts[i].items())\n",
    "\n",
    "            self.dicts[i] = sorted(self.dicts[i], key=lambda x: (-x[1], x[0]))\n",
    "            vocabs, _ = list(zip(*self.dicts[i]))\n",
    "            self.dicts[i] = dict(zip(vocabs, range(1, len(vocabs) + 1)))\n",
    "            self.dicts[i]['<unk>'] = 0\n",
    "\n",
    "    def gen(self, idx, key):\n",
    "        \"\"\"\n",
    "        出现次数低于cutoff的特征,return 0\n",
    "        否则return int(此整数意味着该特征key的类别index中的序号)\n",
    "        \"\"\"\n",
    "        if key not in self.dicts[idx]:     \n",
    "            res = self.dicts[idx]['<unk>']\n",
    "        else:\n",
    "            res = self.dicts[idx][key]\n",
    "        return res\n",
    "\n",
    "    #FIXME: map(func,iteratable) 对于迭代器调用func\n",
    "    #return (26个类别)每个类别下的有效特征数(sparse稀疏)\n",
    "    def dicts_sizes(self):\n",
    "        return list(map(len, self.dicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(datadir, outdir):\n",
    "    \"\"\"\n",
    "    All the 13 integer features are normalzied to continous values and these\n",
    "    continous features are combined into one vecotr with dimension 13.\n",
    "\n",
    "    Each of the 26 categorical features are one-hot encoded and all the one-hot\n",
    "    vectors are combined into one sparse binary vector.\n",
    "    \n",
    "    1.idFeatures 整数特征（稠密？）\n",
    "    2.sparseFeatures 类别特征（稀疏？）\n",
    "    \"\"\"\n",
    "    idFeatures = ContinuousFeatureGenerator(len(continous_features))\n",
    "    idFeatures.build(os.path.join(datadir, 'train.txt'), continous_features)\n",
    "\n",
    "    sparseFeatures = CategoryDictGenerator(len(categorial_features))\n",
    "    sparseFeatures.build(os.path.join(datadir, 'train.txt'), categorial_features, cutoff=200)#200 50\n",
    "\n",
    "    sparseFeatures_sizes = sparseFeatures.dicts_sizes()\n",
    "    categorial_feature_offset = [0]\n",
    "    for i in range(1, len(categorial_features)):\n",
    "        offset = categorial_feature_offset[i - 1] + sparseFeatures_sizes[i - 1]\n",
    "        categorial_feature_offset.append(offset)\n",
    "\n",
    "    random.seed(0)\n",
    "\n",
    "    # 90% of the data are used for training, and 10% of the data are used\n",
    "    # for validation.\n",
    "    train_lgb = open(os.path.join(outdir, 'train_lgb.txt'), 'w')\n",
    "    valid_lgb = open(os.path.join(outdir, 'valid_lgb.txt'), 'w')\n",
    "\n",
    "    with open(os.path.join(outdir, 'train.txt'), 'w') as out_train:\n",
    "        with open(os.path.join(outdir, 'valid.txt'), 'w') as out_valid:\n",
    "            with open(os.path.join(datadir, 'train.txt'), 'r') as f:\n",
    "                for line in f:\n",
    "                    features = line.rstrip('\\n').split('\\t')\n",
    "                    continous_feats = []\n",
    "                    continous_vals = []\n",
    "                    for i in range(0, len(continous_features)):\n",
    "\n",
    "                        val = idFeatures.gen(i, features[continous_features[i]])\n",
    "                        # FIXME continous_vals和continous_feats区别？？\n",
    "                        continous_vals.append(\n",
    "                            \"{0:.6f}\".format(val).rstrip('0').rstrip('.'))\n",
    "                        continous_feats.append(\n",
    "                            \"{0:.6f}\".format(val).rstrip('0').rstrip('.'))#('{0}'.format(val))\n",
    "\n",
    "                    categorial_vals = []\n",
    "                    categorial_lgb_vals = []\n",
    "                    for i in range(0, len(categorial_features)):\n",
    "                        val = sparseFeatures.gen(i, features[categorial_features[i]]) + categorial_feature_offset[i]\n",
    "                        categorial_vals.append(str(val))\n",
    "                        val_lgb = sparseFeatures.gen(i, features[categorial_features[i]])\n",
    "                        categorial_lgb_vals.append(str(val_lgb))\n",
    "\n",
    "                    continous_vals = ','.join(continous_vals)\n",
    "                    categorial_vals = ','.join(categorial_vals)\n",
    "                    label = features[0]\n",
    "                    \n",
    "                    ##### 注意xgboost的输出形式 continous_feats categorial_lgb_vals\n",
    "                    if random.randint(0, 9999) % 10 != 0:\n",
    "                        out_train.write(','.join(\n",
    "                            [continous_vals, categorial_vals, label]) + '\\n')\n",
    "                        \n",
    "                        train_lgb.write('\\t'.join(label) + '\\t')\n",
    "                        train_lgb.write('\\t'.join(continous_feats) + '\\t')\n",
    "                        train_lgb.write('\\t'.join(categorial_lgb_vals) + '\\n')\n",
    "\n",
    "                    else:\n",
    "                        out_valid.write(','.join(\n",
    "                            [continous_vals, categorial_vals, label]) + '\\n')\n",
    "\n",
    "                        valid_lgb.write('\\t'.join(label) + '\\t')\n",
    "                        valid_lgb.write('\\t'.join(continous_feats) + '\\t')\n",
    "                        valid_lgb.write('\\t'.join(categorial_lgb_vals) + '\\n')\n",
    "\n",
    "    train_lgb.close()\n",
    "    valid_lgb.close()\n",
    "\n",
    "    test_lgb = open(os.path.join(outdir, 'test_lgb.txt'), 'w')\n",
    "    with open(os.path.join(outdir, 'test.txt'), 'w') as out:\n",
    "        with open(os.path.join(datadir, 'test.txt'), 'r') as f:\n",
    "            for line in f:\n",
    "                features = line.rstrip('\\n').split('\\t')\n",
    "\n",
    "                continous_feats = []\n",
    "                continous_vals = []\n",
    "                for i in range(0, len(continous_features)):\n",
    "                    val = idFeatures.gen(i, features[continous_features[i] - 1])\n",
    "                    continous_vals.append(\n",
    "                        \"{0:.6f}\".format(val).rstrip('0').rstrip('.'))\n",
    "                    continous_feats.append(\n",
    "                            \"{0:.6f}\".format(val).rstrip('0').rstrip('.'))#('{0}'.format(val))\n",
    "\n",
    "                categorial_vals = []\n",
    "                categorial_lgb_vals = []\n",
    "                for i in range(0, len(categorial_features)):\n",
    "                    val = sparseFeatures.gen(i,\n",
    "                                    features[categorial_features[i] -\n",
    "                                             1]) + categorial_feature_offset[i]\n",
    "                    categorial_vals.append(str(val))\n",
    "\n",
    "                    val_lgb = sparseFeatures.gen(i, features[categorial_features[i] - 1])\n",
    "                    categorial_lgb_vals.append(str(val_lgb))\n",
    "\n",
    "                continous_vals = ','.join(continous_vals)\n",
    "                categorial_vals = ','.join(categorial_vals)\n",
    "\n",
    "                out.write(','.join([continous_vals, categorial_vals]) + '\\n')\n",
    "\n",
    "                                                                \n",
    "                test_lgb.write('\\t'.join(continous_feats) + '\\t')\n",
    "                test_lgb.write('\\t'.join(categorial_lgb_vals) + '\\n')\n",
    "\n",
    "    test_lgb.close()\n",
    "    return sparseFeatures_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e2987bb25863>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minputdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/media/data/cuixuange/Criteo_dataset/rawData\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moutputdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/media/data/cuixuange/Criteo_dataset/xgboost_data\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdict_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputdir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocess' is not defined"
     ]
    }
   ],
   "source": [
    "#inputdir and outputdir\n",
    "inputdir = \"/media/data/cuixuange/Criteo_dataset/rawData\"\n",
    "outputdir = \"/media/data/cuixuange/Criteo_dataset/xgboost_data\"\n",
    "dict_sizes = preprocess(inputdir,outputdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05,0.004983,0.05,0,0.021594,0.008,0.15,0.04,0.362,0.125,0.2,0,0.04,2,86,363,737,1229,1262,1268,2246,2291,2774,2977,3929,4314,5210,5549,6055,6468,6504,7064,7250,7252,7646,7656,7668,8139,8170,0\n",
      "\n",
      "0\t0.05\t0.004983\t0.05\t0\t0.021594\t0.008\t0.15\t0.04\t0.362\t0.125\t0.2\t0\t0.04\t2\t16\t0\t0\t1\t1\t0\t3\t1\t481\t0\t0\t0\t3\t317\t0\t1\t27\t1\t2\t0\t0\t2\t0\t2\t0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##########test\n",
    "#e.g. XGboost input： label,Idfeatures,catoryFeatures(注意：数值为0的分别是怎么来的？？ cutoff)\n",
    "with open(outputdir+\"/train.txt\",\"r\") as f:\n",
    "    for line in f:\n",
    "        print(line)\n",
    "        break\n",
    "with open(outputdir+\"/train_lgb.txt\",\"r\") as f:\n",
    "    for line in f:\n",
    "        print(line)\n",
    "        break\n",
    "        \n",
    "# for category_feature_nums in dict_sizes:\n",
    "#     print(category_feature_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "#输入格式两种: 1.csv 2.libsvm\n",
    "\n",
    "df_train = pd.read_csv(\"/media/data/cuixuange/Criteo_dataset/xgboost_data/train_lgb.txt\", header=None, sep=\"\\t\")\n",
    "df_valid = pd.read_csv(\"/media/data/cuixuange/Criteo_dataset/xgboost_data/valid_lgb.txt\", header=None, sep=\"\\t\")\n",
    "\n",
    "iter_num=32\n",
    "params = {\n",
    "        'task': 'train',\n",
    "        'booster': 'gbtree',\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': ['rmse', 'auc', 'logloss'],\n",
    "    \n",
    "        'max_depth': 5,    #最后一层是32个节点？？？\n",
    "#         'num_trees': 32,    # 没有这个参数  通过num_boost_round控制迭代次数\n",
    "        'max_leaf_nodes':30,\n",
    "        'eta': 0.05,\n",
    "        'colsample_bytree': 0.9,\n",
    "        'subsample': 0.8,\n",
    "#         'bagging_freq': 5,   #xgboost没有这一项？\n",
    "        'verbosity': 0,  \n",
    "        'tree_method':\"exact\"\n",
    "#         \"tree_method\":\"gpu_exact\"  #gpu_hist 尚未安装gpu版本的xgboost\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(899991, 39)\n",
      "[5.0000e-02 4.9830e-03 5.0000e-02 0.0000e+00 2.1594e-02 8.0000e-03\n",
      " 1.5000e-01 4.0000e-02 3.6200e-01 1.2500e-01 2.0000e-01 0.0000e+00\n",
      " 4.0000e-02 2.0000e+00 1.6000e+01 0.0000e+00 0.0000e+00 1.0000e+00\n",
      " 1.0000e+00 0.0000e+00 3.0000e+00 1.0000e+00 4.8100e+02 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 3.0000e+00 3.1700e+02 0.0000e+00 1.0000e+00\n",
      " 2.7000e+01 1.0000e+00 2.0000e+00 0.0000e+00 0.0000e+00 2.0000e+00\n",
      " 0.0000e+00 2.0000e+00 0.0000e+00]\n",
      "[0]\ttrain-rmse:0.492084\ttrain-auc:0.704211\ttrain-logloss:0.677409\tval-rmse:0.492144\tval-auc:0.699668\tval-logloss:0.677563\n",
      "Multiple eval metrics have been passed: 'val-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until val-logloss hasn't improved in 5 rounds.\n",
      "[1]\ttrain-rmse:0.484898\ttrain-auc:0.707748\ttrain-logloss:0.663406\tval-rmse:0.485032\tval-auc:0.702784\tval-logloss:0.663611\n",
      "[2]\ttrain-rmse:0.478221\ttrain-auc:0.709156\ttrain-logloss:0.650404\tval-rmse:0.478431\tval-auc:0.704077\tval-logloss:0.650809\n",
      "[3]\ttrain-rmse:0.472139\ttrain-auc:0.710275\ttrain-logloss:0.638664\tval-rmse:0.472379\tval-auc:0.705133\tval-logloss:0.63916\n",
      "[4]\ttrain-rmse:0.466514\ttrain-auc:0.710995\ttrain-logloss:0.627959\tval-rmse:0.466843\tval-auc:0.70583\tval-logloss:0.628546\n",
      "[5]\ttrain-rmse:0.461375\ttrain-auc:0.712985\ttrain-logloss:0.618107\tval-rmse:0.461746\tval-auc:0.707833\tval-logloss:0.618814\n",
      "[6]\ttrain-rmse:0.456704\ttrain-auc:0.714062\ttrain-logloss:0.60918\tval-rmse:0.457126\tval-auc:0.709161\tval-logloss:0.610005\n",
      "[7]\ttrain-rmse:0.452417\ttrain-auc:0.713955\ttrain-logloss:0.60103\tval-rmse:0.452909\tval-auc:0.709123\tval-logloss:0.601948\n",
      "[8]\ttrain-rmse:0.448501\ttrain-auc:0.715005\ttrain-logloss:0.593537\tval-rmse:0.449024\tval-auc:0.710143\tval-logloss:0.594515\n",
      "[9]\ttrain-rmse:0.444916\ttrain-auc:0.715973\ttrain-logloss:0.586682\tval-rmse:0.445486\tval-auc:0.711218\tval-logloss:0.58772\n",
      "[10]\ttrain-rmse:0.441653\ttrain-auc:0.716557\ttrain-logloss:0.580387\tval-rmse:0.442273\tval-auc:0.711827\tval-logloss:0.58152\n",
      "[11]\ttrain-rmse:0.438701\ttrain-auc:0.717235\ttrain-logloss:0.574678\tval-rmse:0.439363\tval-auc:0.712561\tval-logloss:0.575873\n",
      "[12]\ttrain-rmse:0.43598\ttrain-auc:0.717353\ttrain-logloss:0.569355\tval-rmse:0.43668\tval-auc:0.712734\tval-logloss:0.570631\n",
      "[13]\ttrain-rmse:0.433479\ttrain-auc:0.717705\ttrain-logloss:0.564422\tval-rmse:0.434216\tval-auc:0.713112\tval-logloss:0.565776\n",
      "[14]\ttrain-rmse:0.431218\ttrain-auc:0.717926\ttrain-logloss:0.559943\tval-rmse:0.432005\tval-auc:0.713253\tval-logloss:0.561381\n",
      "[15]\ttrain-rmse:0.429159\ttrain-auc:0.718684\ttrain-logloss:0.555829\tval-rmse:0.429979\tval-auc:0.713959\tval-logloss:0.557332\n",
      "[16]\ttrain-rmse:0.427288\ttrain-auc:0.719107\ttrain-logloss:0.552043\tval-rmse:0.428138\tval-auc:0.714409\tval-logloss:0.553608\n",
      "[17]\ttrain-rmse:0.425554\ttrain-auc:0.71967\ttrain-logloss:0.548499\tval-rmse:0.426425\tval-auc:0.715067\tval-logloss:0.550104\n",
      "[18]\ttrain-rmse:0.423958\ttrain-auc:0.720057\ttrain-logloss:0.545209\tval-rmse:0.424858\tval-auc:0.715457\tval-logloss:0.546867\n",
      "[19]\ttrain-rmse:0.422504\ttrain-auc:0.7205\ttrain-logloss:0.542171\tval-rmse:0.423434\tval-auc:0.71594\tval-logloss:0.543891\n",
      "[20]\ttrain-rmse:0.421179\ttrain-auc:0.720916\ttrain-logloss:0.539374\tval-rmse:0.422135\tval-auc:0.716436\tval-logloss:0.541141\n",
      "[21]\ttrain-rmse:0.419971\ttrain-auc:0.721259\ttrain-logloss:0.536792\tval-rmse:0.420957\tval-auc:0.71677\tval-logloss:0.538614\n",
      "[22]\ttrain-rmse:0.418855\ttrain-auc:0.721547\ttrain-logloss:0.53438\tval-rmse:0.419866\tval-auc:0.717047\tval-logloss:0.536257\n",
      "[23]\ttrain-rmse:0.417834\ttrain-auc:0.722177\ttrain-logloss:0.532158\tval-rmse:0.418871\tval-auc:0.717704\tval-logloss:0.534085\n",
      "[24]\ttrain-rmse:0.4169\ttrain-auc:0.722499\ttrain-logloss:0.5301\tval-rmse:0.417959\tval-auc:0.718069\tval-logloss:0.53207\n",
      "[25]\ttrain-rmse:0.416043\ttrain-auc:0.722764\ttrain-logloss:0.528197\tval-rmse:0.41713\tval-auc:0.718277\tval-logloss:0.530227\n",
      "[26]\ttrain-rmse:0.415249\ttrain-auc:0.723462\ttrain-logloss:0.526422\tval-rmse:0.416351\tval-auc:0.719006\tval-logloss:0.528488\n",
      "[27]\ttrain-rmse:0.414512\ttrain-auc:0.72359\ttrain-logloss:0.524751\tval-rmse:0.415646\tval-auc:0.719097\tval-logloss:0.52688\n",
      "[28]\ttrain-rmse:0.413845\ttrain-auc:0.724038\ttrain-logloss:0.523209\tval-rmse:0.415007\tval-auc:0.71949\tval-logloss:0.525396\n",
      "[29]\ttrain-rmse:0.413242\ttrain-auc:0.724189\ttrain-logloss:0.521792\tval-rmse:0.414421\tval-auc:0.719608\tval-logloss:0.524026\n",
      "[30]\ttrain-rmse:0.412666\ttrain-auc:0.724507\ttrain-logloss:0.520455\tval-rmse:0.413869\tval-auc:0.719887\tval-logloss:0.522736\n",
      "[31]\ttrain-rmse:0.412132\ttrain-auc:0.724905\ttrain-logloss:0.519194\tval-rmse:0.413361\tval-auc:0.720249\tval-logloss:0.521531\n"
     ]
    }
   ],
   "source": [
    "#数据形式需要是Dmatrix （注意xgboost只处理）\n",
    "y_train = df_train[0].values\n",
    "y_valid = df_valid[0].values\n",
    "X_train = df_train.drop(0, axis=1).values\n",
    "X_valid = df_valid.drop(0, axis=1).values\n",
    "print(X_train.shape)\n",
    "print(X_train[0])\n",
    "\n",
    "#类别特征已经使用整数表示  是否还要转化为one-hot编码？（避免整数之间的有序关系影响？）\n",
    "# 决策树按照特征的分布来切分 而非数值大小\n",
    "xgb_train = xgb.DMatrix(data=X_train, label=y_train, feature_names=[\"I1\",\"I2\",\"I3\",\"I4\",\"I5\",\"I6\",\"I7\",\"I8\",\"I9\",\"I10\",\"I11\",\"I12\",\"I13\",\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"C6\",\"C7\",\"C8\",\"C9\",\"C10\",\"C11\",\"C12\",\"C13\",\"C14\",\"C15\",\"C16\",\"C17\",\"C18\",\"C19\",\"C20\",\"C21\",\"C22\",\"C23\",\"C24\",\"C25\",\"C26\"])\n",
    "xgb_eval = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=[\"I1\",\"I2\",\"I3\",\"I4\",\"I5\",\"I6\",\"I7\",\"I8\",\"I9\",\"I10\",\"I11\",\"I12\",\"I13\",\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"C6\",\"C7\",\"C8\",\"C9\",\"C10\",\"C11\",\"C12\",\"C13\",\"C14\",\"C15\",\"C16\",\"C17\",\"C18\",\"C19\",\"C20\",\"C21\",\"C22\",\"C23\",\"C24\",\"C25\",\"C26\"])    \n",
    "\n",
    "watchlist = [(xgb_train,'train'),(xgb_eval,'val')]\n",
    "evals_result_dicts={}\n",
    "gbm = xgb.train(params=params,\n",
    "                dtrain=xgb_train,\n",
    "                evals=watchlist,\n",
    "                num_boost_round=iter_num,#迭代次数等于基学习器的个数\n",
    "                evals_result=evals_result_dicts,\n",
    "                early_stopping_rounds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rmse of prediction is: 0.41336129271495525\n"
     ]
    }
   ],
   "source": [
    "# save model to file\n",
    "gbm.save_model('/media/data/cuixuange/Criteo_dataset/model/xgboost_model.txt')\n",
    "\n",
    "# # predict\n",
    "# ntree_limit=gbm.best_ntree_limit 最好一次的结果\n",
    "y_pred = gbm.predict(xgb_eval, ntree_limit=gbm.best_ntree_limit)\n",
    "\n",
    "\n",
    "# # eval\n",
    "## 在测试集上,计算预测值和真实值的rmse均方根误差\n",
    "print('The rmse of prediction is:', mean_squared_error(y_valid, y_pred) ** 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I11': 1973.414042871286, 'I1': 1896.2753725961543, 'I8': 525.8752960608696, 'I7': 2041.820918677778, 'C9': 298.5788797090909, 'I6': 1427.3076277821656, 'I13': 479.6859143670591, 'I5': 283.5589223075001, 'C25': 120.34583872142859, 'C14': 180.99780204647888, 'I3': 288.2823439855263, 'C17': 169.76483742031246, 'I9': 340.9323533702128, 'I4': 263.5624411773585, 'C24': 138.09528749999998, 'C20': 119.58196934705883, 'C23': 167.74050371891894, 'C2': 103.48684551428572, 'C26': 86.5357973, 'C7': 114.9118499, 'I12': 117.42880256666666, 'I2': 17.1096802, 'C18': 69.55350688333333, 'C3': 111.585938, 'C6': 66.58263651666665, 'I10': 27.9199219, 'C15': 64.9836121, 'C4': 124.3487778, 'C22': 158.071579, 'C13': 84.48830656666668, 'C16': 151.150513, 'C10': 31.821167}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "查看每一个特征的重要程度\n",
    "\"\"\"\n",
    "print(gbm.get_score(importance_type='gain'))\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# 特征的重要性排序\n",
    "# \"\"\"\n",
    "# def ret_feat_impt(gbm):\n",
    "#     gain = gbm.get_score(importance_type='gain').reshape(-1, 1) / sum(gbm.feature_importance(\"gain\"))\n",
    "#     col = np.array(gbm.feature_name()).reshape(-1, 1)\n",
    "#     return sorted(np.column_stack((col, gain)),key=lambda x: x[1],reverse=True)\n",
    "# ret_feat_impt(gbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"GBDT 模型参数的保存\"\"\"\n",
    "import pickle\n",
    "\n",
    "#256次的booster\n",
    "gbm.dump_model(fout=\"/media/data/cuixuange/Criteo_dataset/model/xgboost_dump.parameter\",dump_format=\"json\")\n",
    "# 使用pickle 直接保存对象,省得创建模型对象+load_model\n",
    "#这里选择保存obj=(model,parameter)\n",
    "pickle.dump(gbm, open('{}.pickle'.format('/media/data/cuixuange/Criteo_dataset/model/xgboost_dump'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "print(gbm.best_ntree_limit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tendorflow",
   "language": "python",
   "name": "tendorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
