{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error#均方误差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 13 integer features and 26 categorical features\n",
    "continous_features = range(1, 14)\n",
    "categorial_features = range(14, 40)\n",
    "\n",
    "# 整数的每一个特征的总数的95% clip_point\n",
    "# Clip integer features. The clip point for each integer feature\n",
    "# is derived from the 95% quantile of the total values in each feature\n",
    "continous_clip = [20, 600, 100, 50, 64000, 500, 100, 50, 500, 10, 10, 10, 50]\n",
    "\n",
    "class ContinuousFeatureGenerator:\n",
    "    \"\"\"\n",
    "    Normalize the integer features to [0, 1] by min-max normalization\n",
    "    \"\"\"\n",
    "    def __init__(self, num_feature):\n",
    "        self.num_feature = num_feature\n",
    "        self.min = [sys.maxsize] * num_feature\n",
    "        self.max = [-sys.maxsize] * num_feature\n",
    "\n",
    "    def build(self, datafile, continous_features):\n",
    "        with open(datafile, 'r') as f:\n",
    "            for line in f:\n",
    "                features = line.rstrip('\\n').split('\\t')\n",
    "                for i in range(0, self.num_feature):\n",
    "                    val = features[continous_features[i]]\n",
    "                    if val != '':\n",
    "                        val = int(val)\n",
    "                        if val > continous_clip[i]:\n",
    "                            val = continous_clip[i]\n",
    "                        self.min[i] = min(self.min[i], val)\n",
    "                        self.max[i] = max(self.max[i], val)\n",
    "\n",
    "    def gen(self, idx, val):\n",
    "        if val == '':\n",
    "            return 0.0\n",
    "        val = float(val)\n",
    "        return (val - self.min[idx]) / (self.max[idx] - self.min[idx])\n",
    "\n",
    "class CategoryDictGenerator:\n",
    "    \"\"\"\n",
    "    Generate dictionary for each of the categorical features\n",
    "    \"\"\"\n",
    "    def __init__(self, num_feature):\n",
    "        self.dicts = []\n",
    "        self.num_feature = num_feature\n",
    "        for i in range(0, num_feature):\n",
    "            self.dicts.append(collections.defaultdict(int))\n",
    "\n",
    "    def build(self, datafile, categorial_features, cutoff=0):\n",
    "        \"\"\"\n",
    "        1.统计每个类别下的所有特征出现次数\n",
    "        2.每个类别筛选出现次数大于cutoff的特征; 给这些特征编号\n",
    "        \"\"\"\n",
    "        with open(datafile, 'r') as f:\n",
    "            for line in f:\n",
    "                features = line.rstrip('\\n').split('\\t')\n",
    "                for i in range(0, self.num_feature):\n",
    "                    if features[categorial_features[i]] != '':\n",
    "                        self.dicts[i][features[categorial_features[i]]] += 1\n",
    "        for i in range(0, self.num_feature):\n",
    "            self.dicts[i] = filter(lambda x: x[1] >= cutoff,\n",
    "                                   self.dicts[i].items())\n",
    "\n",
    "            self.dicts[i] = sorted(self.dicts[i], key=lambda x: (-x[1], x[0]))\n",
    "            vocabs, _ = list(zip(*self.dicts[i]))\n",
    "            self.dicts[i] = dict(zip(vocabs, range(1, len(vocabs) + 1)))\n",
    "            self.dicts[i]['<unk>'] = 0\n",
    "\n",
    "    def gen(self, idx, key):\n",
    "        \"\"\"\n",
    "        出现次数低于cutoff的特征,return 0\n",
    "        否则return int(此整数意味着该特征key的类别index中的序号)\n",
    "        \"\"\"\n",
    "        if key not in self.dicts[idx]:     \n",
    "            res = self.dicts[idx]['<unk>']\n",
    "        else:\n",
    "            res = self.dicts[idx][key]\n",
    "        return res\n",
    "\n",
    "    #FIXME: map(func,iteratable) 对于迭代器调用func\n",
    "    #return (26个类别)每个类别下的有效特征数(sparse稀疏)\n",
    "    def dicts_sizes(self):\n",
    "        return list(map(len, self.dicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(datadir, outdir):\n",
    "    \"\"\"\n",
    "    All the 13 integer features are normalzied to continous values and these\n",
    "    continous features are combined into one vecotr with dimension 13.\n",
    "\n",
    "    Each of the 26 categorical features are one-hot encoded and all the one-hot\n",
    "    vectors are combined into one sparse binary vector.\n",
    "    \n",
    "    1.idFeatures 整数特征（稠密？）\n",
    "    2.sparseFeatures 类别特征（稀疏？）\n",
    "    \"\"\"\n",
    "    idFeatures = ContinuousFeatureGenerator(len(continous_features))\n",
    "    idFeatures.build(os.path.join(datadir, 'train.txt'), continous_features)\n",
    "\n",
    "    sparseFeatures = CategoryDictGenerator(len(categorial_features))\n",
    "    sparseFeatures.build(os.path.join(datadir, 'train.txt'), categorial_features, cutoff=200)#200 50\n",
    "\n",
    "    sparseFeatures_sizes = sparseFeatures.dicts_sizes()\n",
    "    categorial_feature_offset = [0]\n",
    "    for i in range(1, len(categorial_features)):\n",
    "        offset = categorial_feature_offset[i - 1] + sparseFeatures_sizes[i - 1]\n",
    "        categorial_feature_offset.append(offset)\n",
    "\n",
    "    random.seed(0)\n",
    "\n",
    "    # 90% of the data are used for training, and 10% of the data are used\n",
    "    # for validation.\n",
    "    train_lgb = open(os.path.join(outdir, 'train_lgb.txt'), 'w')\n",
    "    valid_lgb = open(os.path.join(outdir, 'valid_lgb.txt'), 'w')\n",
    "\n",
    "    with open(os.path.join(outdir, 'train.txt'), 'w') as out_train:\n",
    "        with open(os.path.join(outdir, 'valid.txt'), 'w') as out_valid:\n",
    "            with open(os.path.join(datadir, 'train.txt'), 'r') as f:\n",
    "                for line in f:\n",
    "                    features = line.rstrip('\\n').split('\\t')\n",
    "                    continous_feats = []\n",
    "                    continous_vals = []\n",
    "                    for i in range(0, len(continous_features)):\n",
    "\n",
    "                        val = idFeatures.gen(i, features[continous_features[i]])\n",
    "                        # FIXME continous_vals和continous_feats区别？？\n",
    "                        continous_vals.append(\n",
    "                            \"{0:.6f}\".format(val).rstrip('0').rstrip('.'))\n",
    "                        continous_feats.append(\n",
    "                            \"{0:.6f}\".format(val).rstrip('0').rstrip('.'))#('{0}'.format(val))\n",
    "\n",
    "                    categorial_vals = []\n",
    "                    categorial_lgb_vals = []\n",
    "                    for i in range(0, len(categorial_features)):\n",
    "                        val = sparseFeatures.gen(i, features[categorial_features[i]]) + categorial_feature_offset[i]\n",
    "                        categorial_vals.append(str(val))\n",
    "                        val_lgb = sparseFeatures.gen(i, features[categorial_features[i]])\n",
    "                        categorial_lgb_vals.append(str(val_lgb))\n",
    "\n",
    "                    continous_vals = ','.join(continous_vals)\n",
    "                    categorial_vals = ','.join(categorial_vals)\n",
    "                    label = features[0]\n",
    "                    \n",
    "                    ##### 注意xgboost的输出形式 continous_feats categorial_lgb_vals\n",
    "                    if random.randint(0, 9999) % 10 != 0:\n",
    "                        out_train.write(','.join(\n",
    "                            [continous_vals, categorial_vals, label]) + '\\n')\n",
    "                        \n",
    "                        train_lgb.write('\\t'.join(label) + '\\t')\n",
    "                        train_lgb.write('\\t'.join(continous_feats) + '\\t')\n",
    "                        train_lgb.write('\\t'.join(categorial_lgb_vals) + '\\n')\n",
    "\n",
    "                    else:\n",
    "                        out_valid.write(','.join(\n",
    "                            [continous_vals, categorial_vals, label]) + '\\n')\n",
    "\n",
    "                        valid_lgb.write('\\t'.join(label) + '\\t')\n",
    "                        valid_lgb.write('\\t'.join(continous_feats) + '\\t')\n",
    "                        valid_lgb.write('\\t'.join(categorial_lgb_vals) + '\\n')\n",
    "\n",
    "    train_lgb.close()\n",
    "    valid_lgb.close()\n",
    "\n",
    "    test_lgb = open(os.path.join(outdir, 'test_lgb.txt'), 'w')\n",
    "    with open(os.path.join(outdir, 'test.txt'), 'w') as out:\n",
    "        with open(os.path.join(datadir, 'test.txt'), 'r') as f:\n",
    "            for line in f:\n",
    "                features = line.rstrip('\\n').split('\\t')\n",
    "\n",
    "                continous_feats = []\n",
    "                continous_vals = []\n",
    "                for i in range(0, len(continous_features)):\n",
    "                    val = idFeatures.gen(i, features[continous_features[i] - 1])\n",
    "                    continous_vals.append(\n",
    "                        \"{0:.6f}\".format(val).rstrip('0').rstrip('.'))\n",
    "                    continous_feats.append(\n",
    "                            \"{0:.6f}\".format(val).rstrip('0').rstrip('.'))#('{0}'.format(val))\n",
    "\n",
    "                categorial_vals = []\n",
    "                categorial_lgb_vals = []\n",
    "                for i in range(0, len(categorial_features)):\n",
    "                    val = sparseFeatures.gen(i,\n",
    "                                    features[categorial_features[i] -\n",
    "                                             1]) + categorial_feature_offset[i]\n",
    "                    categorial_vals.append(str(val))\n",
    "\n",
    "                    val_lgb = sparseFeatures.gen(i, features[categorial_features[i] - 1])\n",
    "                    categorial_lgb_vals.append(str(val_lgb))\n",
    "\n",
    "                continous_vals = ','.join(continous_vals)\n",
    "                categorial_vals = ','.join(categorial_vals)\n",
    "\n",
    "                out.write(','.join([continous_vals, categorial_vals]) + '\\n')\n",
    "\n",
    "                                                                \n",
    "                test_lgb.write('\\t'.join(continous_feats) + '\\t')\n",
    "                test_lgb.write('\\t'.join(categorial_lgb_vals) + '\\n')\n",
    "\n",
    "    test_lgb.close()\n",
    "    return sparseFeatures_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e2987bb25863>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minputdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/media/data/cuixuange/Criteo_dataset/rawData\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moutputdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/media/data/cuixuange/Criteo_dataset/xgboost_data\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdict_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputdir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocess' is not defined"
     ]
    }
   ],
   "source": [
    "#inputdir and outputdir\n",
    "inputdir = \"/media/data/cuixuange/Criteo_dataset/rawData\"\n",
    "outputdir = \"/media/data/cuixuange/Criteo_dataset/xgboost_data\"\n",
    "dict_sizes = preprocess(inputdir,outputdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05,0.004983,0.05,0,0.021594,0.008,0.15,0.04,0.362,0.125,0.2,0,0.04,2,86,363,737,1229,1262,1268,2246,2291,2774,2977,3929,4314,5210,5549,6055,6468,6504,7064,7250,7252,7646,7656,7668,8139,8170,0\n",
      "\n",
      "0\t0.05\t0.004983\t0.05\t0\t0.021594\t0.008\t0.15\t0.04\t0.362\t0.125\t0.2\t0\t0.04\t2\t16\t0\t0\t1\t1\t0\t3\t1\t481\t0\t0\t0\t3\t317\t0\t1\t27\t1\t2\t0\t0\t2\t0\t2\t0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##########test\n",
    "#e.g. XGboost input： label,Idfeatures,catoryFeatures(注意：数值为0的分别是怎么来的？？ cutoff)\n",
    "with open(outputdir+\"/train.txt\",\"r\") as f:\n",
    "    for line in f:\n",
    "        print(line)\n",
    "        break\n",
    "with open(outputdir+\"/train_lgb.txt\",\"r\") as f:\n",
    "    for line in f:\n",
    "        print(line)\n",
    "        break\n",
    "        \n",
    "# for category_feature_nums in dict_sizes:\n",
    "#     print(category_feature_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "#输入格式两种: 1.csv 2.libsvm\n",
    "\n",
    "df_train = pd.read_csv(\"/media/data/cuixuange/Criteo_dataset/xgboost_data/train_lgb.txt\", header=None, sep=\"\\t\")\n",
    "df_valid = pd.read_csv(\"/media/data/cuixuange/Criteo_dataset/xgboost_data/valid_lgb.txt\", header=None, sep=\"\\t\")\n",
    "\n",
    "iter_num=32\n",
    "params = {\n",
    "        'task': 'train',\n",
    "        'booster': 'gbtree',\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': ['rmse', 'auc', 'logloss'],\n",
    "    \n",
    "        'max_depth': 5,    #最后一层是32个节点？？？\n",
    "#         'num_trees': 32,    # 没有这个参数  通过num_boost_round控制迭代次数\n",
    "        'max_leaf_nodes':30,\n",
    "        'eta': 0.05,\n",
    "        'colsample_bytree': 0.9,\n",
    "        'subsample': 0.8,\n",
    "#         'bagging_freq': 5,   #xgboost没有这一项？\n",
    "        'verbosity': 0,  \n",
    "        'tree_method':\"exact\"\n",
    "#         \"tree_method\":\"gpu_exact\"  #gpu_hist 尚未安装gpu版本的xgboost\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(899991, 39)\n",
      "[5.0000e-02 4.9830e-03 5.0000e-02 0.0000e+00 2.1594e-02 8.0000e-03\n",
      " 1.5000e-01 4.0000e-02 3.6200e-01 1.2500e-01 2.0000e-01 0.0000e+00\n",
      " 4.0000e-02 2.0000e+00 1.6000e+01 0.0000e+00 0.0000e+00 1.0000e+00\n",
      " 1.0000e+00 0.0000e+00 3.0000e+00 1.0000e+00 4.8100e+02 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 3.0000e+00 3.1700e+02 0.0000e+00 1.0000e+00\n",
      " 2.7000e+01 1.0000e+00 2.0000e+00 0.0000e+00 0.0000e+00 2.0000e+00\n",
      " 0.0000e+00 2.0000e+00 0.0000e+00]\n",
      "[0]\ttrain-rmse:0.491988\ttrain-auc:0.709839\ttrain-logloss:0.677292\tval-rmse:0.492058\tval-auc:0.70533\tval-logloss:0.677379\n",
      "Multiple eval metrics have been passed: 'val-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until val-logloss hasn't improved in 5 rounds.\n",
      "[1]\ttrain-rmse:0.484696\ttrain-auc:0.714723\ttrain-logloss:0.662941\tval-rmse:0.484832\tval-auc:0.709599\tval-logloss:0.66323\n",
      "[2]\ttrain-rmse:0.477924\ttrain-auc:0.715683\ttrain-logloss:0.649871\tval-rmse:0.478136\tval-auc:0.710607\tval-logloss:0.650247\n",
      "[3]\ttrain-rmse:0.471719\ttrain-auc:0.716641\ttrain-logloss:0.637892\tval-rmse:0.472009\tval-auc:0.711194\tval-logloss:0.638447\n",
      "[4]\ttrain-rmse:0.46602\ttrain-auc:0.717091\ttrain-logloss:0.626997\tval-rmse:0.466391\tval-auc:0.711564\tval-logloss:0.627693\n",
      "[5]\ttrain-rmse:0.460804\ttrain-auc:0.718581\ttrain-logloss:0.617064\tval-rmse:0.461243\tval-auc:0.713084\tval-logloss:0.617862\n",
      "[6]\ttrain-rmse:0.456061\ttrain-auc:0.718912\ttrain-logloss:0.608\tval-rmse:0.456566\tval-auc:0.713505\tval-logloss:0.608938\n",
      "[7]\ttrain-rmse:0.451698\ttrain-auc:0.72014\ttrain-logloss:0.599685\tval-rmse:0.452235\tval-auc:0.714994\tval-logloss:0.60067\n",
      "[8]\ttrain-rmse:0.447712\ttrain-auc:0.720943\ttrain-logloss:0.592053\tval-rmse:0.448308\tval-auc:0.715727\tval-logloss:0.593156\n",
      "[9]\ttrain-rmse:0.444111\ttrain-auc:0.721632\ttrain-logloss:0.585151\tval-rmse:0.444746\tval-auc:0.716561\tval-logloss:0.586317\n",
      "[10]\ttrain-rmse:0.440786\ttrain-auc:0.721981\ttrain-logloss:0.578748\tval-rmse:0.441467\tval-auc:0.717021\tval-logloss:0.579989\n",
      "[11]\ttrain-rmse:0.437775\ttrain-auc:0.722491\ttrain-logloss:0.572894\tval-rmse:0.4385\tval-auc:0.717502\tval-logloss:0.574231\n",
      "[12]\ttrain-rmse:0.434985\ttrain-auc:0.722903\ttrain-logloss:0.567451\tval-rmse:0.435745\tval-auc:0.71802\tval-logloss:0.568842\n",
      "[13]\ttrain-rmse:0.432445\ttrain-auc:0.723286\ttrain-logloss:0.562448\tval-rmse:0.433249\tval-auc:0.718412\tval-logloss:0.563919\n",
      "[14]\ttrain-rmse:0.430143\ttrain-auc:0.723664\ttrain-logloss:0.557881\tval-rmse:0.430997\tval-auc:0.71869\tval-logloss:0.559441\n",
      "[15]\ttrain-rmse:0.42804\ttrain-auc:0.72392\ttrain-logloss:0.553659\tval-rmse:0.428936\tval-auc:0.718871\tval-logloss:0.555303\n",
      "[16]\ttrain-rmse:0.426122\ttrain-auc:0.724379\ttrain-logloss:0.549778\tval-rmse:0.427047\tval-auc:0.719427\tval-logloss:0.551477\n",
      "[17]\ttrain-rmse:0.424363\ttrain-auc:0.724815\ttrain-logloss:0.546181\tval-rmse:0.42532\tval-auc:0.719895\tval-logloss:0.547941\n",
      "[18]\ttrain-rmse:0.422748\ttrain-auc:0.725268\ttrain-logloss:0.542852\tval-rmse:0.423743\tval-auc:0.720325\tval-logloss:0.544681\n",
      "[19]\ttrain-rmse:0.421271\ttrain-auc:0.725777\ttrain-logloss:0.539756\tval-rmse:0.422294\tval-auc:0.72086\tval-logloss:0.541648\n",
      "[20]\ttrain-rmse:0.419908\ttrain-auc:0.726194\ttrain-logloss:0.536882\tval-rmse:0.420962\tval-auc:0.721353\tval-logloss:0.538827\n",
      "[21]\ttrain-rmse:0.418672\ttrain-auc:0.726625\ttrain-logloss:0.534231\tval-rmse:0.419759\tval-auc:0.721734\tval-logloss:0.536242\n",
      "[22]\ttrain-rmse:0.41753\ttrain-auc:0.726944\ttrain-logloss:0.531756\tval-rmse:0.418648\tval-auc:0.722015\tval-logloss:0.533832\n",
      "[23]\ttrain-rmse:0.416467\ttrain-auc:0.72766\ttrain-logloss:0.529442\tval-rmse:0.417603\tval-auc:0.722771\tval-logloss:0.531558\n",
      "[24]\ttrain-rmse:0.415521\ttrain-auc:0.727984\ttrain-logloss:0.527339\tval-rmse:0.416679\tval-auc:0.723142\tval-logloss:0.5295\n",
      "[25]\ttrain-rmse:0.414644\ttrain-auc:0.728216\ttrain-logloss:0.525377\tval-rmse:0.41583\tval-auc:0.723314\tval-logloss:0.527601\n",
      "[26]\ttrain-rmse:0.413846\ttrain-auc:0.72857\ttrain-logloss:0.52356\tval-rmse:0.415059\tval-auc:0.723663\tval-logloss:0.525839\n",
      "[27]\ttrain-rmse:0.413095\ttrain-auc:0.728827\ttrain-logloss:0.521841\tval-rmse:0.414338\tval-auc:0.723863\tval-logloss:0.524186\n",
      "[28]\ttrain-rmse:0.412398\ttrain-auc:0.729326\ttrain-logloss:0.520233\tval-rmse:0.413663\tval-auc:0.724371\tval-logloss:0.522622\n",
      "[29]\ttrain-rmse:0.411784\ttrain-auc:0.729478\ttrain-logloss:0.518789\tval-rmse:0.413073\tval-auc:0.724492\tval-logloss:0.521234\n",
      "[30]\ttrain-rmse:0.411217\ttrain-auc:0.729835\ttrain-logloss:0.517444\tval-rmse:0.41252\tval-auc:0.724866\tval-logloss:0.519922\n",
      "[31]\ttrain-rmse:0.410672\ttrain-auc:0.730191\ttrain-logloss:0.516145\tval-rmse:0.411995\tval-auc:0.725189\tval-logloss:0.518673\n"
     ]
    }
   ],
   "source": [
    "#数据形式需要是Dmatrix （注意xgboost只处理）\n",
    "y_train = df_train[0].values\n",
    "y_valid = df_valid[0].values\n",
    "X_train = df_train.drop(0, axis=1).values\n",
    "X_valid = df_valid.drop(0, axis=1).values\n",
    "print(X_train.shape)\n",
    "print(X_train[0])\n",
    "\n",
    "#类别特征已经使用整数表示  是否还要转化为one-hot编码？（避免整数之间的有序关系影响？）\n",
    "# 决策树按照特征的分布来切分 而非数值大小\n",
    "xgb_train = xgb.DMatrix(data=X_train, label=y_train, feature_names=[\"I1\",\"I2\",\"I3\",\"I4\",\"I5\",\"I6\",\"I7\",\"I8\",\"I9\",\"I10\",\"I11\",\"I12\",\"I13\",\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"C6\",\"C7\",\"C8\",\"C9\",\"C10\",\"C11\",\"C12\",\"C13\",\"C14\",\"C15\",\"C16\",\"C17\",\"C18\",\"C19\",\"C20\",\"C21\",\"C22\",\"C23\",\"C24\",\"C25\",\"C26\"])\n",
    "xgb_eval = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=[\"I1\",\"I2\",\"I3\",\"I4\",\"I5\",\"I6\",\"I7\",\"I8\",\"I9\",\"I10\",\"I11\",\"I12\",\"I13\",\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"C6\",\"C7\",\"C8\",\"C9\",\"C10\",\"C11\",\"C12\",\"C13\",\"C14\",\"C15\",\"C16\",\"C17\",\"C18\",\"C19\",\"C20\",\"C21\",\"C22\",\"C23\",\"C24\",\"C25\",\"C26\"])    \n",
    "\n",
    "watchlist = [(xgb_train,'train'),(xgb_eval,'val')]\n",
    "evals_result_dicts={}\n",
    "gbm = xgb.train(params=params,\n",
    "                dtrain=xgb_train,\n",
    "                evals=watchlist,\n",
    "                num_boost_round=iter_num,#迭代次数等于基学习器的个数\n",
    "                evals_result=evals_result_dicts,\n",
    "                early_stopping_rounds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rmse of prediction is: 0.4119949802317371\n"
     ]
    }
   ],
   "source": [
    "# save model to file\n",
    "gbm.save_model('/media/data/cuixuange/Criteo_dataset/model/xgboost_model.txt')\n",
    "\n",
    "# # predict\n",
    "# ntree_limit=gbm.best_ntree_limit 最好一次的结果\n",
    "y_pred = gbm.predict(xgb_eval, ntree_limit=gbm.best_ntree_limit)\n",
    "\n",
    "\n",
    "# # eval\n",
    "## 在测试集上,计算预测值和真实值的rmse均方根误差\n",
    "print('The rmse of prediction is:', mean_squared_error(y_valid, y_pred) ** 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I11': 1428.951213182143, 'I1': 1409.1892370469443, 'I8': 305.8931632657471, 'I7': 1527.7021724249996, 'C9': 186.76292754444447, 'C23': 96.59272583699997, 'I6': 1002.7175735968285, 'I13': 303.08045438034947, 'C7': 59.80811584782608, 'I5': 136.02166216776703, 'C25': 80.3793927576087, 'C2': 60.36467494705882, 'C14': 95.71104377362636, 'I3': 145.98547949431946, 'C18': 52.745481670588234, 'C24': 61.54558010909091, 'C17': 110.71008439327733, 'I9': 170.2615961915315, 'I4': 184.16804658260872, 'I12': 63.84120720194445, 'C20': 63.126253089189184, 'I2': 25.425209624375, 'C11': 17.231822235000003, 'C4': 63.41161501294118, 'C6': 50.92944414090909, 'C12': 50.33730708, 'C13': 36.8681209, 'C16': 71.09249264, 'C3': 21.186661903333334, 'C26': 48.87206676625001, 'C22': 65.01264411818181, 'I10': 91.436251775, 'C21': 49.1305542, 'C15': 77.31409185833333, 'C1': 6.6306057}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "查看每一个特征的重要程度\n",
    "\"\"\"\n",
    "print(gbm.get_score(importance_type='gain'))\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# 特征的重要性排序\n",
    "# \"\"\"\n",
    "# def ret_feat_impt(gbm):\n",
    "#     gain = gbm.get_score(importance_type='gain').reshape(-1, 1) / sum(gbm.feature_importance(\"gain\"))\n",
    "#     col = np.array(gbm.feature_name()).reshape(-1, 1)\n",
    "#     return sorted(np.column_stack((col, gain)),key=lambda x: x[1],reverse=True)\n",
    "# ret_feat_impt(gbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"GBDT 模型参数的保存\"\"\"\n",
    "import pickle\n",
    "\n",
    "#256次的booster\n",
    "gbm.dump_model(fout=\"/media/data/cuixuange/Criteo_dataset/model/xgboost_dump.parameter\",dump_format=\"json\")\n",
    "# 使用pickle 直接保存对象,省得创建模型对象+load_model\n",
    "#这里选择保存obj=(model,parameter)\n",
    "pickle.dump(gbm, open('{}.pickle'.format('/media/data/cuixuange/Criteo_dataset/model/xgboost_dump'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "print(gbm.best_ntree_limit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tendorflow",
   "language": "python",
   "name": "tendorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
